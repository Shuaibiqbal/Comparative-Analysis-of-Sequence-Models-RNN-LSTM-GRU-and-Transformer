{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6103e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819965f",
   "metadata": {},
   "source": [
    "# Why RNN, LSTM, GRU, and Transformers?\n",
    "\n",
    "## **Recurrent Neural Networks (RNN)**\n",
    "RNNs were developed to handle sequential data by maintaining a \"memory\" of previous inputs in the sequence. They are good for tasks like time series prediction or text generation where past information influences future predictions.\n",
    "\n",
    "**However, RNNs have limitations:**\n",
    "\n",
    "**Vanishing Gradient Problem:** During backpropagation, gradients can get very small, causing learning to stop as you go further back in the sequence. This makes training RNNs on long sequences very hard.\n",
    "\n",
    "## **Long Short-Term Memory (LSTM)**\n",
    "LSTM was introduced to address the vanishing gradient problem in RNNs. It uses a special memory cell structure that allows the network to \"remember\" information for long periods and is much more effective for longer sequences.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "While LSTMs mitigate the vanishing gradient problem, they still have some issues with long-range dependencies and are computationally expensive.\n",
    "\n",
    "## **Gated Recurrent Unit (GRU)**\n",
    "GRU is a simplified version of LSTM. It combines the forget and input gates into one, which makes it faster to train and requires fewer parameters.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "GRUs might not handle very long sequences as well as LSTMs due to fewer gates.\n",
    "\n",
    "## **Transformers**\n",
    "Transformers solve the problem of long-range dependencies. Instead of relying on sequential processing, transformers use self-attention mechanisms that allow the model to weigh all parts of the sequence at once. This makes transformers faster and better at handling very long sequences. They are currently the state-of-the-art for most sequence modeling tasks, including NLP tasks like machine translation.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Transformers can be very computationally expensive because of their attention mechanism, which needs to evaluate pairwise relations between every token in the sequence.\n",
    "\n",
    "**Code Setup**\n",
    "We’ll use the sklearn.datasets and work with the 20 Newsgroups dataset, which is often used for text classification. We'll preprocess it into sequences suitable for RNN, LSTM, GRU, and Transformer models.\n",
    "\n",
    "We’ll split the dataset into three sets: Train, Validation (Eval), and Test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe9fba",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "We'll first load the dataset, vectorize it using TfidfVectorizer, and then pad the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b0e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "506fb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "newsgroups = fetch_20newsgroups(subset=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad76a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = newsgroups.data\n",
    "y = newsgroups.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70123f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(X_raw).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758eddfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07e61ba0",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "## What is TF-IDF?\n",
    "\n",
    "**TF-IDF** is a statistical measure used in text analysis and natural language processing (NLP) to evaluate how important a word is in a document relative to a collection of documents (also called a corpus). It combines two components:\n",
    "\n",
    "### 1. Term Frequency (TF):\n",
    "This measures how frequently a term (word) appears in a document. It helps indicate the relative importance of a term within the document.\n",
    "\n",
    "**Formula:**\n",
    "TF(t)= Number of times word \"t\" appears / Total number of words in the document\n",
    "\n",
    "\n",
    "\n",
    "### 2. Inverse Document Frequency (IDF):\n",
    "This measures how important a term is across the entire corpus (collection of documents). Words that appear in many documents have lower IDF values, while words that are unique to fewer documents have higher IDF values. This helps reduce the importance of words that are too common (like \"the,\" \"and,\" etc.).\n",
    "\n",
    "**Formula:**\n",
    "IDF(t) = log((Total number of documents) / (Number of documents containing term t))\n",
    "\n",
    "### 3. TF-IDF:\n",
    "The final value is the product of the Term Frequency and the Inverse Document Frequency, helping determine the weight of each word in a document relative to the entire corpus.\n",
    "\n",
    "**Formula:**\n",
    "TF-IDF(t) = TF(t) * IDF(t)\n",
    "\n",
    "## When to Use TF-IDF:\n",
    "\n",
    "- **Text Classification:** When building models to classify text data into categories, TF-IDF helps identify important words (features) that can differentiate between classes.\n",
    "- **Information Retrieval:** TF-IDF is used in search engines to rank documents based on relevance to a search query.\n",
    "- **Feature Extraction:** In NLP, when you want to convert text data into a numerical format for machine learning models, TF-IDF is commonly used.\n",
    "- **Reducing Noise:** Common words (e.g., \"the,\" \"is,\" \"to\") are usually given low weights, reducing their impact on models.\n",
    "\n",
    "## How TF-IDF is Applied to the 20 Newsgroups Dataset:\n",
    "\n",
    "### The 20 Newsgroups Dataset:\n",
    "This dataset contains 20 different categories of newsgroup posts. Some of the categories are:\n",
    "\n",
    "- **alt.atheism**\n",
    "- **comp.graphics**\n",
    "- **rec.autos**\n",
    "- **sci.med**\n",
    "- **talk.politics.misc**\n",
    "- ... and others.\n",
    "\n",
    "Each newsgroup post is a piece of text, and the goal is to classify each post into one of the 20 categories. To do that, we need to convert the text into a format that a machine learning model can understand, which is where TF-IDF comes in.\n",
    "\n",
    "### How TF-IDF Transforms the Dataset:\n",
    "\n",
    "#### Term Frequency (TF):\n",
    "For each post in the dataset, the **TF** part of the formula measures how many times each word appears in that specific post. For example:\n",
    "\n",
    "If the word \"graphics\" appears 5 times in a post and the post contains 100 words, the **TF** for \"graphics\" in that post would be:\n",
    "TF(\"graphics\") = 5/100 = 0.05\n",
    "\n",
    "This means \"graphics\" contributes 5% of the total words in that document.\n",
    "\n",
    "#### Inverse Document Frequency (IDF):\n",
    "The **IDF** component adjusts the weight of words that are common across all the newsgroup posts. Words like \"the,\" \"and,\" \"is\" will appear in many documents and thus will have a low IDF value.\n",
    "\n",
    "For instance, if the word \"the\" appears in almost every newsgroup post, the **IDF** for \"the\" will be small, which means it will have a low weight and not significantly influence the classification.\n",
    "\n",
    "If a word appears in only a few categories (like \"graphics\" in the **comp.graphics** category), its **IDF** value will be higher, meaning it is more distinctive and useful for classification.\n",
    "\n",
    "#### Combining TF and IDF (TF-IDF):\n",
    "The final **TF-IDF** value for a word in a document is the product of its **TF** and **IDF**. Words that are frequent in a specific post but rare across the entire corpus (like \"graphics\" in **comp.graphics**) will have a high **TF-IDF** score, making them highly informative for classification.\n",
    "\n",
    "### Example of How TF-IDF Works in the 20 Newsgroups Dataset:\n",
    "Let’s say you have the following three documents in the dataset:\n",
    "\n",
    "- **Document 1** (from **comp.graphics**): \"Graphics hardware is important in modern computing.\"\n",
    "- **Document 2** (from **rec.autos**): \"The importance of graphics in automobile design.\"\n",
    "- **Document 3** (from **sci.med**): \"Medical graphics can be used in medical research.\"\n",
    "\n",
    "For each document, TF calculates the frequency of terms:\n",
    "\n",
    "- In **Document 1**, \"graphics\" may appear 1 time, and other words like \"hardware\" and \"modern\" also have their frequencies.\n",
    "- In **Document 2**, \"graphics\" appears 1 time, and similar calculations are done for other words.\n",
    "\n",
    "The **IDF** component checks how many documents contain the word \"graphics\" and assigns it a weight. If \"graphics\" appears in many documents, its **IDF** will be lower, as it is common. However, if \"graphics\" appears mainly in **comp.graphics** and not much elsewhere, the **IDF** will be higher.\n",
    "\n",
    "Finally, the **TF-IDF** values are calculated by multiplying the term frequency (**TF**) by the inverse document frequency (**IDF**). Words like \"graphics\" will have a high **TF-IDF** score in **comp.graphics**, making them more important for classifying the document into that category.\n",
    "\n",
    "## What Happens in Your Code:\n",
    "```python\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(X).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a8679fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9d720a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "# Convert text to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7de130c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.0551521 , 0.06228945, 0.05398663, 0.03889093,\n",
       "       0.12335451, 0.02971408, 0.01860012, 0.13315578, 0.03173198,\n",
       "       0.06314308, 0.07767828, 0.08378101, 0.05066146, 0.16870275,\n",
       "       0.11411294, 0.08328202, 0.14112828, 0.09147512, 0.06531498,\n",
       "       0.18464977, 0.1083335 , 0.04455032, 0.05699943, 0.15994526,\n",
       "       0.07580327, 0.11806278, 0.02067249, 0.01571676, 0.16034569,\n",
       "       0.0619455 , 0.06772143, 0.09961367, 0.02395684, 0.08101771,\n",
       "       0.04431971, 0.02890425, 0.0532248 , 0.03675459, 0.04002432,\n",
       "       0.09919647, 0.22413785, 0.09168329, 0.03407714, 0.0788684 ,\n",
       "       0.05893586, 0.07678712, 0.05146762, 0.01576353, 0.08181758,\n",
       "       0.10915665, 0.05937229, 0.09655792, 0.08303835, 0.04310689,\n",
       "       0.05764678, 0.02907696, 0.05918566, 0.0252772 , 0.1425931 ,\n",
       "       0.07032004, 0.01634109, 0.03397822, 0.077074  , 0.56144282,\n",
       "       0.07223526, 0.08788553, 0.05473728, 0.02819857, 0.07891483,\n",
       "       0.10608441, 0.0609065 , 0.05467748, 0.10321891, 0.07531502,\n",
       "       0.15727657, 0.09774466, 0.07740841, 0.14968295, 0.04348784,\n",
       "       0.04216837, 0.08291791, 0.04708058, 0.03403429, 0.0866973 ,\n",
       "       0.04991666, 0.07632951, 0.16802472, 0.03031677, 0.04411012,\n",
       "       0.05761077, 0.08723166, 0.04620952, 0.04314505, 0.0297222 ,\n",
       "       0.07591813, 0.04353183, 0.0248629 , 0.07674656, 0.0243462 ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X[0])[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329da7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
