{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6103e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819965f",
   "metadata": {},
   "source": [
    "# Why RNN, LSTM, GRU, and Transformers?\n",
    "\n",
    "## **Recurrent Neural Networks (RNN)**\n",
    "RNNs were developed to handle sequential data by maintaining a \"memory\" of previous inputs in the sequence. They are good for tasks like time series prediction or text generation where past information influences future predictions.\n",
    "\n",
    "**However, RNNs have limitations:**\n",
    "\n",
    "**Vanishing Gradient Problem:** During backpropagation, gradients can get very small, causing learning to stop as you go further back in the sequence. This makes training RNNs on long sequences very hard.\n",
    "\n",
    "## **Long Short-Term Memory (LSTM)**\n",
    "LSTM was introduced to address the vanishing gradient problem in RNNs. It uses a special memory cell structure that allows the network to \"remember\" information for long periods and is much more effective for longer sequences.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "While LSTMs mitigate the vanishing gradient problem, they still have some issues with long-range dependencies and are computationally expensive.\n",
    "\n",
    "## **Gated Recurrent Unit (GRU)**\n",
    "GRU is a simplified version of LSTM. It combines the forget and input gates into one, which makes it faster to train and requires fewer parameters.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "GRUs might not handle very long sequences as well as LSTMs due to fewer gates.\n",
    "\n",
    "## **Transformers**\n",
    "Transformers solve the problem of long-range dependencies. Instead of relying on sequential processing, transformers use self-attention mechanisms that allow the model to weigh all parts of the sequence at once. This makes transformers faster and better at handling very long sequences. They are currently the state-of-the-art for most sequence modeling tasks, including NLP tasks like machine translation.\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "Transformers can be very computationally expensive because of their attention mechanism, which needs to evaluate pairwise relations between every token in the sequence.\n",
    "\n",
    "**Code Setup**\n",
    "We’ll use the sklearn.datasets and work with the 20 Newsgroups dataset, which is often used for text classification. We'll preprocess it into sequences suitable for RNN, LSTM, GRU, and Transformer models.\n",
    "\n",
    "We’ll split the dataset into three sets: Train, Validation (Eval), and Test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe9fba",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "We'll first load the dataset, vectorize it using TfidfVectorizer, and then pad the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b0e370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 12:54:35.776904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745913275.788930 1395970 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745913275.792624 1395970 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-29 12:54:35.805100: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "506fb224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset \n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "X_raw = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train_raw, X_temp, y_train, y_temp = train_test_split(X_raw, y, test_size=0.4, random_state=42)\n",
    "X_val_raw, X_test_raw, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e61ba0",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "## What is TF-IDF?\n",
    "\n",
    "**TF-IDF** is a statistical measure used in text analysis and natural language processing (NLP) to evaluate how important a word is in a document relative to a collection of documents (also called a corpus). It combines two components:\n",
    "\n",
    "### 1. Term Frequency (TF):\n",
    "This measures how frequently a term (word) appears in a document. It helps indicate the relative importance of a term within the document.\n",
    "\n",
    "**Formula:**\n",
    "TF(t)= Number of times word \"t\" appears / Total number of words in the document\n",
    "\n",
    "\n",
    "\n",
    "### 2. Inverse Document Frequency (IDF):\n",
    "This measures how important a term is across the entire corpus (collection of documents). Words that appear in many documents have lower IDF values, while words that are unique to fewer documents have higher IDF values. This helps reduce the importance of words that are too common (like \"the,\" \"and,\" etc.).\n",
    "\n",
    "**Formula:**\n",
    "IDF(t) = log((Total number of documents) / (Number of documents containing term t))\n",
    "\n",
    "### 3. TF-IDF:\n",
    "The final value is the product of the Term Frequency and the Inverse Document Frequency, helping determine the **weight of each word** in a document relative to the entire corpus.\n",
    "\n",
    "**Formula:**\n",
    "TF-IDF(t) = TF(t) * IDF(t)\n",
    "\n",
    "## When to Use TF-IDF:\n",
    "\n",
    "- **Text Classification:** When building models to classify text data into categories, TF-IDF helps identify important words (features) that can differentiate between classes.\n",
    "- **Information Retrieval:** TF-IDF is used in search engines to rank documents based on relevance to a search query.\n",
    "- **Feature Extraction:** In NLP, when you want to convert text data into a numerical format for machine learning models, TF-IDF is commonly used.\n",
    "- **Reducing Noise:** Common words (e.g., \"the,\" \"is,\" \"to\") are usually given low weights, reducing their impact on models.\n",
    "\n",
    "## How TF-IDF is Applied to the 20 Newsgroups Dataset:\n",
    "\n",
    "### The 20 Newsgroups Dataset:\n",
    "This dataset contains 20 different categories of newsgroup posts. Some of the categories are:\n",
    "\n",
    "- **alt.atheism**\n",
    "- **comp.graphics**\n",
    "- **rec.autos**\n",
    "- **sci.med**\n",
    "- **talk.politics.misc**\n",
    "- ... and others.\n",
    "\n",
    "Each newsgroup post is a piece of text, and the goal is to classify each post into one of the 20 categories. To do that, we need to convert the text into a format that a machine learning model can understand, which is where TF-IDF comes in.\n",
    "\n",
    "### How TF-IDF Transforms the Dataset:\n",
    "\n",
    "#### Term Frequency (TF):\n",
    "For each post in the dataset, the **TF** part of the formula measures how many times each word appears in that specific post. For example:\n",
    "\n",
    "If the word \"graphics\" appears 5 times in a post and the post contains 100 words, the **TF** for \"graphics\" in that post would be:\n",
    "TF(\"graphics\") = 5/100 = 0.05\n",
    "\n",
    "This means \"graphics\" contributes 5% of the total words in that document.\n",
    "\n",
    "#### Inverse Document Frequency (IDF):\n",
    "The **IDF** component adjusts the weight of words that are common across all the newsgroup posts. Words like \"the,\" \"and,\" \"is\" will appear in many documents and thus will have a low IDF value.\n",
    "\n",
    "For instance, if the word \"the\" appears in almost every newsgroup post, the **IDF** for \"the\" will be small, which means it will have a low weight and not significantly influence the classification.\n",
    "\n",
    "If a word appears in only a few categories (like \"graphics\" in the **comp.graphics** category), its **IDF** value will be higher, meaning it is more distinctive and useful for classification.\n",
    "\n",
    "#### Combining TF and IDF (TF-IDF):\n",
    "The final **TF-IDF** value for a word in a document is the product of its **TF** and **IDF**. Words that are frequent in a specific post but rare across the entire corpus (like \"graphics\" in **comp.graphics**) will have a high **TF-IDF** score, making them highly informative for classification.\n",
    "\n",
    "### Example of How TF-IDF Works in the 20 Newsgroups Dataset:\n",
    "Let’s say you have the following three documents in the dataset:\n",
    "\n",
    "- **Document 1** (from **comp.graphics**): \"Graphics hardware is important in modern computing.\"\n",
    "- **Document 2** (from **rec.autos**): \"The importance of graphics in automobile design.\"\n",
    "- **Document 3** (from **sci.med**): \"Medical graphics can be used in medical research.\"\n",
    "\n",
    "For each document, TF calculates the frequency of terms:\n",
    "\n",
    "- In **Document 1**, \"graphics\" may appear 1 time, and other words like \"hardware\" and \"modern\" also have their frequencies.\n",
    "- In **Document 2**, \"graphics\" appears 1 time, and similar calculations are done for other words.\n",
    "\n",
    "The **IDF** component checks how many documents contain the word \"graphics\" and assigns it a weight. If \"graphics\" appears in many documents, its **IDF** will be lower, as it is common. However, if \"graphics\" appears mainly in **comp.graphics** and not much elsewhere, the **IDF** will be higher.\n",
    "\n",
    "Finally, the **TF-IDF** values are calculated by multiplying the term frequency (**TF**) by the inverse document frequency (**IDF**). Words like \"graphics\" will have a high **TF-IDF** score in **comp.graphics**, making them more important for classifying the document into that category.\n",
    "\n",
    "## What Happens in Your Code:\n",
    "```python\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "TfidfVectorizer(max_features=10000): This limits the vocabulary to the top 10,000 words based on their TF-IDF scores. These words are considered the most informative in the dataset for classification purposes.\n",
    "\n",
    "fit_transform(X): This applies the TF-IDF transformation to the data (X), converting the raw text into a matrix of TF-IDF features. Each document is now represented as a vector of numerical values, where each value corresponds to the TF-IDF score for a word in the document.\n",
    "\n",
    "toarray(): Converts the sparse matrix (which only stores non-zero values to save memory) into a dense array, which is easier to work with but takes more memory.\n",
    "\n",
    "**Summary:**\n",
    "TF-IDF helps convert text data into numerical vectors that represent the importance of words in the context of the dataset.\n",
    "\n",
    "It reduces the influence of common words that don’t provide much insight into the category of the document (like \"the,\" \"is,\" \"and\").\n",
    "\n",
    "It highlights more unique and distinctive words that are valuable for text classification (like \"graphics\" in the comp.graphics category).\n",
    "\n",
    "Using TF-IDF in the 20 Newsgroups dataset enables you to represent each newsgroup post in a way that a machine learning algorithm can use to predict its category based on the important features (words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31804cc",
   "metadata": {},
   "source": [
    "### Why is Embedding Used, Even If We Have TF-IDF?\n",
    "**1. TF-IDF vs Embedding Layer:**\n",
    "\n",
    "TF-IDF is a statistical representation of a document. It assigns each word in the document a weight (importance) based on how often it appears in the document relative to its frequency across the entire corpus. The result is a sparse vector representation, which is good for text classification tasks, but does not capture semantic relationships between words.\n",
    "\n",
    "For example, TF-IDF would give the words \"king\" and \"queen\" very different weights, even though they are semantically similar.\n",
    "\n",
    "Embeddings, on the other hand, provide dense, continuous vectors that capture semantic meaning. \"King\" and \"queen\" would be represented by similar vectors because they are related concepts (e.g., royalty).\n",
    "\n",
    "**2. When We Use an Embedding Layer:**\n",
    "\n",
    "The Embedding layer is typically used when you're dealing with raw text data, and you want to learn the best representations (embeddings) for words during the training process.\n",
    "\n",
    "TF-IDF is a feature extraction technique, not a model itself. After converting your raw text into a TF-IDF matrix, you're feeding it directly to the model. The Embedding layer, however, is used in models where you want to learn word representations (vectors) that improve during training, which is particularly useful for deep learning models like LSTM, GRU, etc.\n",
    "\n",
    "**Example to Illustrate:**\n",
    "\n",
    "**Using TF-IDF:**\n",
    "\n",
    "You convert your text into a sparse matrix where each word has a frequency or weight. This matrix can be fed into a machine learning model like logistic regression or a neural network.\n",
    "\n",
    "This doesn't capture relationships like \"king\" and \"queen\" being similar.\n",
    "\n",
    "**Using an Embedding Layer:**\n",
    "\n",
    "When using a deep learning model, you might start by converting the words into indices (e.g., 0 for \"king\", 1 for \"queen\", etc.), then use the Embedding layer to transform these indices into dense vectors (e.g., [0.32, 0.15, -0.67, ...]).\n",
    "\n",
    "These dense vectors are learned by the model during training. So, the model learns to represent words like \"king\" and \"queen\" with similar vectors because they have similar meanings.\n",
    "\n",
    "**When to Use Which:**\n",
    "\n",
    "Use TF-IDF when you want a quick, non-learned representation of your text for shallow machine learning models (like SVM, Naive Bayes, etc.).\n",
    "\n",
    "Use the Embedding layer when you want your model to learn word representations and capture complex semantic relationships between words (useful in deep learning models like LSTM, GRU, Transformers, etc.).\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "TF-IDF gives you a static, sparse representation of words based on their frequency.\n",
    "\n",
    "Embedding layers give you a dynamic, dense representation of words that captures their semantic meaning and is learned by the model during training.\n",
    "\n",
    "If you're using deep learning models (like LSTM, GRU, etc.), you generally use Embeddings to get rich word representations, which are beneficial for learning the context in sequences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0c4c4",
   "metadata": {},
   "source": [
    "## 🔸 1. TF-IDF + RNN/LSTM/GRU\n",
    "### 🔹 TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ed5856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi all,\\n\\nI think the subject says it all - does anyone know how to take the rgb/h/vsync from a standard vga connector and record them on video tape??\\n\\nAny help is appreciated!\\n\\n\\nMark J Cargill'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_raw[0] #just for check form of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56a4ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_raw).toarray()\n",
    "X_val_tfidf = vectorizer.fit_transform(X_val_raw).toarray()\n",
    "X_test_tfidf = vectorizer.fit_transform(X_test_raw).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad5772fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11307, 10000), (3769, 10000), (3770, 10000))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "168215b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11307, 10000, 1), (3769, 10000, 1), (3770, 10000, 1))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping\n",
    "X_train_tfidf_reshaped = np.expand_dims(X_train_tfidf, axis=-1)\n",
    "X_val_tfidf_reshaped = np.expand_dims(X_val_tfidf, axis=-1)\n",
    "X_test_tfidf_reshaped = np.expand_dims(X_test_tfidf, axis=-1)\n",
    "X_train_tfidf_reshaped.shape, X_val_tfidf_reshaped.shape, X_test_tfidf_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52976b3",
   "metadata": {},
   "source": [
    "## 3. RNN Model for Text Classification (Using .add() Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cee07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(input_dim=10000, output_dim = 128, input_length=500))\n",
    "# Adding the embedding layer (used if you are working with raw text and want to learn word embeddings)\n",
    "# In this case, we skip the Embedding Layer since you're using TF-IDF vectors already\n",
    "# rnn_model.add(Embedding(input_dim=10000, output_dim=128, input_length=500))\n",
    "rnn_model.add(SimpleRNN(128, return_sequences=False))\n",
    "rnn_model.add(Dense(20, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f60ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn_model.fit(X_train,y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ecde82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape your data to 3D for RNN\n",
    "# X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "\n",
    "# Check the shape\n",
    "print(X_train.shape)  # (num_samples, sequence_length, 1)\n",
    "print(X_val.shape)    # (num_samples, sequence_length, 1)\n",
    "\n",
    "# Define the model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(SimpleRNN(128, input_shape=(X_train.shape[1], 1)))  # Ensure input shape matches the data\n",
    "rnn_model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "rnn_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdad89a",
   "metadata": {},
   "source": [
    "**What do the parameters mean?**\n",
    "\n",
    "Parameter\t                            Meaning\n",
    "\n",
    "**input_dim=10000**\t--> Size of the vocabulary. That is, the total number of unique words your model expects. Here, it assumes 10,000 different words. Each word will have its own vector.\n",
    "\n",
    "Number of unique tokens (words) in whole dataset\n",
    "\n",
    "**output_dim=128** -->\tSize of the vector for each word. That is, each word will be represented by a dense vector of 128 numbers.\n",
    "\n",
    "Number of tokens in each input sequence (padded/truncated)\n",
    "\n",
    "**input_length=500** --> Length of each input sequence (number of words). Here, each input (sentence/document) will have 500 words (after padding/truncating).\n",
    "\n",
    "Size of the dense vector for each word\n",
    "\n",
    "\n",
    "**In simple words:**\n",
    "\n",
    "* You have 10,000 different words.\n",
    "\n",
    "* Each word will be converted into a 128-dimensional vector.\n",
    "\n",
    "* Each input document must have exactly 500 words (either padded or cut).\n",
    "\n",
    "So the output shape of the Embedding layer will be:\n",
    "\n",
    "(batch_size, 500, 128)\n",
    "\n",
    "→ meaning batch_size documents,\n",
    "\n",
    "each document has 500 words,\n",
    "\n",
    "each word is represented by a 128-length vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fb33d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
